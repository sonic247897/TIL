> # Part 1: 프로그램의 구조와 실행
>
> 컴퓨터 시스템에 대한 탐구는 프로세서와 메모리 서브시스템으로 구성된 컴퓨터 자체를 공부하는 것으로 시작한다.
>
> - 응용 프로그램이 어떻게 표시되고 실행되는지에 대한 깊은 이해를 할 수 있게 된다.
> - 안전하고 안정적인 프로그램을 작성하고, 컴퓨팅 자원을 가장 잘 사용할 수 있도록 하는 기법을 배운다.

# 2장. 정보의 표현과 처리

`디지털 혁명` 이후, 현대의 컴퓨터는 **두 개의 값을 갖는 신호로 표현되는 정보(=`비트`)**를 저장하고 처리한다.

인간에게는 십진수 표기를 사용하는 것이 당연한 것이지만, **정보의 `저장`과 `처리`를 위한 기계**를 만들 때는 이진수 값들이 더 잘 동작한다.

- 두 개의 값을 갖는 신호를 저장하고 계산하기 위한 전자회로는 매우 간단하고 안정적이기 때문이다.

**목표**

- 여러 인코딩의 기본 정의로 시작해서,
  - 세 개의 가장 중요한 숫자 표현 - 비부호형 인코딩, 2의 보수 인코딩, 부동소수점 인코딩
- 표시 가능한 숫자의 범위,
  - 컴퓨터에서는 하나의 수를 인코딩하기 위해 **제한된** 개수의 비트를 사용한다. 그래서 결과값이 표시할 수 없을 정도로 큰 경우 `오버플로우`를 발생시킬 수 있다.
    - 정수 연산
      - 양수들 간의 곱셈 결과가 너무 크면 음수가 될 수 있다.
      - 곱셈에서 `교환법칙`과 `분배법칙`이 성립한다.
    - 부동소수점 연산
      - **양수들 간의 곱은 항상 양수**이지만, 오버플로우인 경우 특별한 값인 `+∞`를 만들어 낸다. 
      - 부동소수점 연산은 수의 제한된 정밀도 때문에 `교환법칙`이 성립하지 않는다.
- 비트수준 표시방법,
  - 비트수준 표시방법을 직접적으로 조작하여 **산술연산을 하는 여러 가지 방법**을 도출한다.
  - 산술식 계산 성능을 최적화하기 위해 `컴파일러`가 만들어 내는 기계어 코드를 이해하는 데 중요하다.
- 산술연산 성질 같은 특성을 도출한다.
  - 정수와 부동소수점 연산에서의 서로 다른 수학 특성은 그들이 **표시 방법의 유한성을 어떻게 처리하는지**에 기인한다.
  - **정수 표현은 비교적 작은 범위의 값을 인코딩하지만 매우 정밀하게 하는 반면, 부동소수점 표시는 넓은 범위의 값을 근사값으로만 인코딩해야 한다.**

> 이러한 내용은 **핵심 수학법칙**에 근거하여 설명된다.
>
> 프로그래머는 컴퓨터 산술연산이 어떻게 보다 익숙한 정수와 소수 연산에 연관되는지 완벽한 이해를 해야 하기 때문에 이러한 내용들을 이처럼 `추상적인 관점`에서 살펴보는 것이 중요하다.

``` c
*C++ 언어는 C언어를 기초로 완전히 동일한 숫자 표현과 연산을 사용하여 만들어졌다.
따라서 C언어에 대한 언급한 모든 내용은 C++에도 적용된다.
    =>광범위한 구현을 할 수 있도록 설계
*반면 자바 언어는 숫자 표현과 연산을 위해 새로운 표준을 만들었다.
    =>특정한 형식과 인코딩을 사용한다.
```

## 1. 정보의 저장

대부분의 컴퓨터들은 메모리의 비트에 접근할 때, 메모리에서 주소지정이 가능한 최소단위인 **바이트 단위**로 접근하는 방식을 사용한다.

**기계수준의 프로그램**은 메모리를 `가상메모리`라고 하는 거대한 바이트의 배열로 취급한다.

메모리의 각 바이트는 **주소**라고 하는 고유한 숫자로 식별할 수 있으며, 모든 가능한 주소들의 집합을 `가상 주소공간`이라고 부른다.

> 이름처럼 가상 주소공간은 **기계수준 프로그램에게 제공되는** 개념적인 이미지에 불과하다.
>
> 실제 구현은 DRAM, 플래시메모리, 디스크, 기타 하드웨어와 운영체제 소프트웨어로 이루어진다.

다음 장들에서는 어떻게 `컴파일러`와 `런타임 시스템`이 가상메모리 공간을 **분할**하고, 프로그램 객체(program object)들, 즉 프로그램 데이터, 인스트럭션, 제어정보 등을 **저장**하는지에 대해 다룰 것이다.

- 이러한 관리기법들은 모두 가상 주소공간 내에서 수행된다.
- C 컴파일러의 예시
  - C 컴파일러는 값의 타입에 따라 다른 기계수준 코드를 생성할 수 있다.
  - 비록 C 컴파일러가 타입 정보를 관리하지만, 컴파일러가 생성하는 실제 기계수준 프로그램은 데이터 타입에 대한 정보를 전혀 가지고 있지 않다.

### 1) 16진수 표시

C에서 0x나 OX로 시작하는 숫자 상수들은 16진수로 해석한다.

> 기계수준 프로그램으로 하게 되는 일반적인 작업은 비트 패턴을 십진, 이진, 16진수 간에 수동으로 변환하는 것이다.

- 이진수와 16진수 간의 변환은 한 번에 하나의 16진수 숫자에 대해 실행할 수 있으므로 즉각적으로 이루어진다.
  - 16진수 -> 이진수: 해당하는 십진수를 생각하여 이진수로 변환한다.
  - 이진수 -> 16진수: 4비트씩 나누어 16진수로 변환한다.
- 십진수와 16진수 표시 간의 변환은 **곱셈**과 **나눗셈**을 사용해야 한다.
  - 십진수 -> 16진수: x를 16으로 반복해서 나누고, 나머지를 나타내는 16진수 숫자를 least significant digit으로 사용한다.
  - 16진수 -> 십진수: 16진수 숫자들을 적절한 16의 제곱수로 곱해야 한다.

### 2) 데이터의 크기

모든 컴퓨터는 **워드 크기(word size)**를 규격으로 가지게 되는데, 이것은 `포인터`의 정규 크기를 표시한다.

> 하나의 가상주소가 한 개의 워드로 인코딩되기 때문에, **워드 크기가 결정하는 가장 중요한 시스템 변수는 `가상 주소공간의 최대 크기`**이다.
>
> - 즉, w비트 워드 크기를 갖는 컴퓨터에서 가상주소는 0에서 2^w - 1 범위를 가지며
> - 프로그램은 최대 2^w 바이트(`= 가상 주소공간의 크기*바이트`)에 접근할 수 있게 된다.

<img src="images/02_virtual_memory.JPG" style="zoom:70%;" />

*바이트(Byte)는 메모리에서 주소지정이 가능한 최소단위이다.*

- 32비트 워드 크기는 가상 주소공간의 크기를 4 GB(= 2^32 Byte), 약 4*(10^9) 바이트로 한정한다.
- 64비트 워드 크기는 16 Exabyte, 즉 1.84*(10^19) 바이트의 가상 주소공간으로 확장되었다.

대부분의 64비트 컴퓨터들은 역방향 호환성을 가지고 있어서 32비트 머신들을 위해 컴파일 된 프로그램들도 실행할 수 있다.



ISO C99는 컴파일러와 컴퓨터 설정에 관계없이 데이터의 크기가 고정된 자료형들을 제안하였다.

- int32_t : 4바이트
- int64_t : 8바이트

> 고정된 정수형 크기를 이용하는 것이 프로그래머들이 **데이터의 표현을 안전하게 통제하는 최상의 방법**이다.

포인터는 프로그램의 워드 크기를 이용한다.

### 3) 주소지정과 바이트 순서

여러 바이트에 걸쳐 있는 **프로그램 객체**들에 대한 `관습` (거의 모든 컴퓨터에서 설정됨)

1. 무엇이 객체의 주소가 되어야 하는지

   : 사용된 바이트의 **최소 주소**

2. 메모리에 바이트들을 어떻게 정렬해야 하는지

   : 멀티 바이트 객체는 **연속된 바이트**에 저장됨



**어떤 객체를 나타내는 바이트들을 정렬**하는 데는 두 가지 일반적인 `관습`이 존재한다.

<img src="images/02_bit.JPG" style="zoom:67%;" />

w가 8의 배수라면 이 비트들은 바이트들로 나눌 수 있다.

- 가장 중요한 바이트(most significant byte)

  : [X*w-1*, X*w-2*, . . . , X*w-8*]

- 가장 덜 중요한 바이트(least significant byte)

  : [X*7*, X*6*, . . . , X*0*]

1. 리틀 엔디안

   : 객체를 메모리에 least significant byte부터 저장한다.

   - 대부분의 인텔 호환 머신들

2. 빅 엔디안

   : 객체를 메모리에 most significant byte부터 저장한다.

   - 대부분의 IBM과 Oracle 머신들

- "대부분" => 관례들은 회사라는 경계로 정확하게 나누어지지 않는다.

> 많은 최신 마이크로프로세서 칩들은 두 가지 방식으로 동작이 가능하도록 구성할 수 있다.
>
> 그러나 일단 특정 운영체제가 결정되면 바이트 순서는 고정되는 게 일반적이다.

ex) int형의 변수 x가 주소 0x100에 있으며, 16진수 값 0x01234567을 갖는다고 하자.

주소 범위 0x100 ~ 0x103까지의 바이트들의 순서는 컴퓨터의 타입에 따라 달라진다.

<img src="images/02_endian.JPG" style="zoom:70%;" />

> 위에서 **주소지정**과 **바이트 순서**에 대해 `관습`이라고 했던 이유는, 다른 관습 대신에 선택해야 하는 기술적 이유는 없기 때문이다.
>
> 하나의 관습이 선택되고 일정하게 지켜지는 한 선택은 상관이 없다.

그러나 때때로 바이트 순서가 이슈가 되기도 한다.

1. 이진 데이터가 **네트워크**를 통해 다른 컴퓨터로 전송될 때

   : 문제는 리틀 엔디안 컴퓨터에서 만들어진 데이터를 빅 엔디안 컴퓨터에 보내야 해서, 수신 측 프로그램에서는 워드들 내 바이트의 순서가 뒤바뀌는 경우이다.

   - `네트워크 응용프로그램`으로 작성된 코드는 송신 측 컴퓨터가 내부 표시를 **네트워크 표준**으로 변경하고,
   - 수신 측 컴퓨터가 **네트워크 표준**을 자신의 내부 표시방식으로 변환하도록 하는 `관습`을 따라야 한다.

2. 기계수준 프로그램에서 **정수 데이터**를 나타내는 바이트들을 해석할 때

   : 리틀 엔디안 컴퓨터를 위해 생성한 기계수준 프로그램을 해석할 때, 정수 데이터를 나타내는 바이트의 순서를 뒤집어야 실제 표현하고자 하는 정수를 알 수 있다.

   - `4004d3: 01 05 43 0b 20 00	add	%eax, 0x200b43(%rip)`

     이 라인은 인텔x86-64 프로세서용 기계수준 코드로, 역어셈블러에 의해 만들어졌다.

   - 이 바이트들의 마지막 4바이트인 `43 0b 20 00`을 **역순으로 `00 20 0b 43`라고 써야 원래 정수를 사람이 해석할 수 있다**. = 0x200b43

3. 프로그램이 정상적인 타입 체계를 회피하도록 작성되었을 때

   : 이러한 코딩 기술은 대부분의 응용 프로그램 작성시에 엄격히 제한되어야 하지만, 시스템-수준 프로그램에서는 상당히 유용하고 심지어 꼭 필요하기까지 하다.

   - C 언어에서 `캐스트(cast)`나 `유니온(union)`을 사용해서, 객체가 만들어졌을 때와는 **다른 타입의 데이터로 참조**될 수 있도록 할 수 있다.
   
   - 아래 코드의 예시는, C 프로그램의 멀티 바이트 객체들인 int, float, void *가 저장하고 있는 `값(멀티 바이트)`을 인간이 읽기 쉽고 이해하기 쉬운 **16진수 형식**으로 출력하는 코드이다. 
     
  => 그러나 빅 엔디안과 리틀 엔디안의 출력값이 달라서 의도하지 못한 결과가 나올 수 있다.
     
     - 그러기 위해서 각 1 바이트의 `비트패턴`을 **unsigned char 인코딩(1장에 의하면 컨텍스트)**을 이용해서 해석하면 `음이 아닌 정수`로 **해석**된다. 
     
       (실제 저장된 값과는 다르다. int형은 2의 보수법 인코딩을 사용하고 unsigned char형은 비부호형 인코딩을 사용하므로, 음의 정수를 양의 정수로 해석한다.)
     
     - 그 결과를 printf를 이용하여 16진수로 출력한다.
   
   ``` c
   #include <stdio.h>
   
   typedef unsigned char* byte_pointer;
   
   void show_bytes(byte_pointer start, size_t len){ // len은 바이트 수
       type_t
       for(i = 0; i < len; ++i)
           printf(" %.2x", start[i]);
       printf("\n");
   }
   
   void show_int(int x){
       // 이 캐스트는 이 포인터가 본래의 데이터 타입을 갖는 하나의 '객체를 가리키는 것이 아니라'
       // '일련의 바이트들을 가리킨다는 것'을 컴파일러에게 지시한다.
       show_bytes((byte_pointer) &x, sizeof(int));
   }
   
   void show_float(float x){
    show_bytes((byte_pointer) &x, sizeof(float));
   }

   void show_int(void* x){
       show_bytes((byte_pointer) &x, sizeof(void*));
   }
   ```
   
   - show_int의 경우 (워드크기= 32bit 컴퓨터, 리틀 엔디언)
   
     <img src="images/02_example_int.JPG" style="zoom:80%;" />
     
   - 빅 엔디언인 경우 출력은 `00 20 0b 43`이 된다.
   
   > int와 float 데이터의 바이트들은 **바이트 순서**를 제외하고는 모든 컴퓨터에서 동일한 결과를 얻었다.
   >
   > 반면에 포인터 값들은 완전히 다른 결과를 얻었다.
   >
   > 서로 다른 컴퓨터=운영체제 구성은 **저장장치의 할당**에 있어서 서로 다른 `관습`을 사용하기 때문이다.

### 4) 스트링의 표시

C에서 스트링은 `null(값 0을 갖는)` 문자로 종료하는 문자열로 인코딩된다.

<u>각 문자</u>는 표준 인코딩에 따라 표시되며, 이 중 가장 일반적인 인코딩이 **ASCII 문자코드**이다.

=> 스트링 타입은 각 문자별로 ASCII 인코딩을 하는데, **문자 인코딩은 멀티 바이트가 아니므로 바이트 순서 이슈가 발생하지 않는다**.  

> 따라서 **ASCII를 문자코드로 사용하는 모든 컴퓨터에서** 바이트 순서나 워드 크기와 무관하게 show_bytes의 결과가 똑같다.
>
> ex) show_bytes("12345", 6) 의 결과 = 31 32 33 34 35 00
>
> - 그 결과 텍스트 데이터는 이진 데이터보다 `플랫폼 독립적`이다.

### 5) 코드의 표현

이진 코드(기계어 코드, `인스트럭션 인코딩`)는 컴퓨터와 운영체제들의 여러 가지 조합들 간에 호환성을 갖는 경우가 드물다.

``` c
int sum(int x, int y){
    return x+y;
}
```

위와 같은 C 함수를 여러 컴퓨터들에서 컴파일하면, 각자 다른 바이트 표시를 갖는 기계어 코드를 생성한다.

> 컴퓨터 시스템에서 근본 개념은 컴퓨터의 관점에서 볼 때 프로그램이라는 것은 단순히 바이트의 연속이라는 것이다.
>
> 컴퓨터는 디버깅 시 도움을 주기 위해 관리하는 일부의 **보조용 표**를 제외하고는 본래의 소스 프로그램에 대한 정보를 전혀 가지고 있지 않다.

``` c

```

