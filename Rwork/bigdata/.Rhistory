contentdata <- readLines(url_val[page], encoding = "UTF-8")
start = which(str_detect(contentdata, "post_content"))
end = which(str_detect(contentdata, "post_ccls"))
#정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data, collapse = "")
content_filter_data <- gsub("<.*?>", "", content_filter_data)
content <- gsub("\t|&nbsp;","",content_filter_data)
# 벡터니까 c로 삽입입해야 함
# 기존에 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist, content_filter_data)
## cat: 디버깅!
cat("\n", page)
}
# 페이지가 바뀔때마다 arra list같은 것에 final_data를 쌓아야 한다.
final_data <- cbind(title, git, url_val, contentlist)
# 기존에 final_data_list에 있던 것과 연결하면 된다(rbind로 밑으로 붙여준다)
final_data_list <- rbind(final_data_list, final_data)
cat("\n", i)
}
#### csv파일로 생성 ####
write.csv(final_data_list, "final_data.csv")
save(final_data_list, file = "final_data.RData")
#### mongodb에 저장하기 ####
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
final_data_list = NULL
for(i in 0:9){
# 페이지마다 연결할 주소가 달라지므로 변수로 처리
myurl <- paste0("https://www.clien.net/service/group/community?&od=T31&po=",i)
url_data <- readLines(myurl, encoding = "UTF-8")
#### title 추출 ####
final_filter_data <- url_data[str_detect(url_data, "subject_fixed")]
title <- str_extract(final_filter_data, "(?<=\">).*(?=</span>)")
#### hit 추출 ####
hit_data <- url_data[str_detect(url_data, "<span class=\"hit\">")]
hit <- str_extract(hit_data, "(?<=\">).*(?=</span>)")[-1]
#### url 추출 ####
str_detect(url_data, "subject_fixed")
myurl <- url_data[which(str_detect(url_data, "subject_fixed"))-3]
url_val <- str_extract(myurl, "(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val, end= -3)
url_val <- paste0("https://www.clien.net",url_val)
########## url을 이용해서 content항목 추출 ########
contentlist = NULL #최초 변수 선언시 null로 초기화
for(page in 1:length(url_val)){
#데이터 가져오기
contentdata <- readLines(url_val[page], encoding = "UTF-8")
start = which(str_detect(contentdata, "post_content"))
end = which(str_detect(contentdata, "post_ccls"))
#정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data, collapse = "")
content_filter_data <- gsub("<.*?>", "", content_filter_data)
content <- gsub("\t|&nbsp;","",content_filter_data)
# 벡터니까 c로 삽입입해야 함
# 기존에 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist, content_filter_data)
## cat: 디버깅!
cat("\n", page)
}
# 페이지가 바뀔때마다 arra list같은 것에 final_data를 쌓아야 한다.
final_data <- cbind(title, git, url_val, contentlist)
# 기존에 final_data_list에 있던 것과 연결하면 된다(rbind로 밑으로 붙여준다)
final_data_list <- rbind(final_data_list, final_data)
cat("\n", i)
}
#### csv파일로 생성 ####
write.csv(final_data_list, "final_data.csv")
save(final_data_list, file = "final_data.RData")
#### mongodb에 저장하기 ####
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
##### 모두의 광장의 1페이지:10페이지의 모든 게시글 크롤링 ####
# 1. 모든 페이지의 title, hit, url, content 추출하기
# 2. crawl_result.csv, crawl_result.RData 저장
# 3. mongodb 저장 (300개 저장)
# 4. for, if문을 활용
library(mongolite) #라이브러리 올릴 때는 큰따옴표 없어도 인식
library("stringr")
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
# 0번부터 9번 페이지까지 반복 작업
final_data_list = NULL
for(i in 0:9){
# 페이지마다 연결할 주소가 달라지므로 변수로 처리
myurl <- paste0("https://www.clien.net/service/group/community?&od=T31&po=",i)
url_data <- readLines(myurl, encoding = "UTF-8")
#### title 추출 ####
final_filter_data <- url_data[str_detect(url_data, "subject_fixed")]
title <- str_extract(final_filter_data, "(?<=\">).*(?=</span>)")
#### hit 추출 ####
hit_data <- url_data[str_detect(url_data, "<span class=\"hit\">")]
hit <- str_extract(hit_data, "(?<=\">).*(?=</span>)")[-1]
#### url 추출 ####
str_detect(url_data, "subject_fixed")
myurl <- url_data[which(str_detect(url_data, "subject_fixed"))-3]
url_val <- str_extract(myurl, "(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val, end= -3)
url_val <- paste0("https://www.clien.net",url_val)
########## url을 이용해서 content항목 추출 ########
contentlist = NULL #최초 변수 선언시 null로 초기화
for(page in 1:length(url_val)){
#데이터 가져오기
contentdata <- readLines(url_val[page], encoding = "UTF-8")
start = which(str_detect(contentdata, "post_content"))
end = which(str_detect(contentdata, "post_ccls"))
#정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data, collapse = "")
content_filter_data <- gsub("<.*?>", "", content_filter_data)
content <- gsub("\t|&nbsp;","",content_filter_data)
# 벡터니까 c로 삽입입해야 함
# 기존에 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist, content_filter_data)
## cat: 디버깅!
cat("\n", page)
}
# 페이지가 바뀔때마다 arra list같은 것에 final_data를 쌓아야 한다.
final_data <- cbind(title, hit, url_val, contentlist)
# 기존에 final_data_list에 있던 것과 연결하면 된다(rbind로 밑으로 붙여준다)
final_data_list <- rbind(final_data_list, final_data)
cat("\n", i)
}
#### csv파일로 생성 ####
write.csv(final_data_list, "final_data.csv")
save(final_data_list, file = "final_data.RData")
#### mongodb에 저장하기 ####
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
##### 모두의 광장의 1페이지:10페이지의 모든 게시글 크롤링 ####
# 1. 모든 페이지의 title, hit, url, content 추출하기
# 2. crawl_result.csv, crawl_result.RData 저장
# 3. mongodb 저장 (300개 저장)
# 4. for, if문을 활용
library(mongolite) #라이브러리 올릴 때는 큰따옴표 없어도 인식
library("stringr")
con <- mongo(collection = "crawl",
db = "bigdata",
url = "mongodb://127.0.0.1")
# 0번부터 9번 페이지까지 반복 작업
final_data_list = NULL
for(i in 0:9){
# 페이지마다 연결할 주소가 달라지므로 변수로 처리
myurl <- paste0("https://www.clien.net/service/group/community?&od=T31&po=",i)
url_data <- readLines(myurl, encoding = "UTF-8")
#### title 추출 ####
final_filter_data <- url_data[str_detect(url_data, "subject_fixed")]
title <- str_extract(final_filter_data, "(?<=\">).*(?=</span>)")
#### hit 추출 ####
hit_data <- url_data[str_detect(url_data, "<span class=\"hit\">")]
hit <- str_extract(hit_data, "(?<=\">).*(?=</span>)")[-1]
#### url 추출 ####
str_detect(url_data, "subject_fixed")
myurl <- url_data[which(str_detect(url_data, "subject_fixed"))-3]
url_val <- str_extract(myurl, "(?<=href=\").*(?=data-role)")
url_val <- str_sub(url_val, end= -3)
url_val <- paste0("https://www.clien.net",url_val)
########## url을 이용해서 content항목 추출 ########
contentlist = NULL #최초 변수 선언시 null로 초기화
content_filter_data = NULL
for(page in 1:length(url_val)){
#데이터 가져오기
contentdata <- readLines(url_val[page], encoding = "UTF-8")
start = which(str_detect(contentdata, "post_content"))
end = which(str_detect(contentdata, "post_ccls"))
#정제
content_filter_data <- contentdata[start:end]
content_filter_data <- paste(content_filter_data, collapse = "")
content_filter_data <- gsub("<.*?>", "", content_filter_data)
content <- gsub("\t|&nbsp;","",content_filter_data)
# 벡터니까 c로 삽입입해야 함
# 기존에 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist, content_filter_data)
## cat: 디버깅!
cat("\n", page)
}
# 페이지가 바뀔때마다 arra list같은 것에 final_data를 쌓아야 한다.
final_data <- cbind(title, hit, url_val, contentlist)
# 기존에 final_data_list에 있던 것과 연결하면 된다(rbind로 밑으로 붙여준다)
final_data_list <- rbind(final_data_list, final_data)
cat("\n", i)
}
#### csv파일로 생성 ####
write.csv(final_data_list, "final_data.csv")
save(final_data_list, file = "final_data.RData")
#### mongodb에 저장하기 ####
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
#### csv파일로 생성 ####
write.csv(final_data_list, "final_data.csv")
save(final_data_list, file = "final_data.RData")
#### mongodb에 저장하기 ####
if(con$count()>0){
con$drop()
}
final_mongo_data <- data.frame(final_data_list)
con$insert(final_mongo_data)
install.packages("N2H4")
library(N2H4)
comments <- getAllComment(url)
comments
comments <- getAllComment(url)
install.packages("N2H4")
library(N2H4)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
comments <- getAllComment(url)
comments
getAllComment(url) %>%
select(username, contents) -> comments #변수에 대입식 거꾸로 사용 가능
getAllComment(url) %>%
select(userName, contents) -> comments #변수에 대입식 거꾸로 사용 가능
getAllComment(url) %>%
select(userName, contents) -> mydata #변수에 대입식 거꾸로 사용 가능
mydata
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
library(N2H4)
library(stringr)
library(dplyr) #체이닝 연산자
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
getAllComment(url) %>%
select(userName, contents) -> mydata #변수에 대입식 거꾸로 사용 가능
mydata
mycomment <- mydata$contents
mycomment
#css의 선택자를 활용할 수 있는 방법 - rvest
ininstall.packages("rvest")
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
readPage <- read_html(url)
readPage
readPage <- read_html(url)
readPage %>%
# 이런 이름으로 되어 있는 모든 tag를 뽑아옴
html_nodes("span.subject_fixed") -> title_data  # DOM기반: node=tag
title_data
readPage %>%
# 이런 이름으로 되어 있는 모든 tag를 뽑아옴
html_nodes("span.subject_fixed") %>%
html_text() -> title_data  # DOM기반: node=tag
title_data
library(rvest)
url <- "http://www.yes24.com/searchcorner/Search?keywordAd=&keyword=&qdomain=%c0%fc%c3%bc&query=%c0%da%b9%d9&domain=BOOK&disp_no=001001003&scode=007_001"
readPage <- read_html(url)
readPage %>%
html_nodes("a.strong") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name.strong") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name.a") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_attr("a") %>%
html_text() -> title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_attr("href") %>%
html_text() -> title_data
title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_nodes('a')
readPage %>%
html_nodes("p.goods_name") %>%
html_nodes('a')
title_data
html_attr(css="strong") %>%
html_text() -> title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_nodes('a')
title_data
readPage %>%
html_nodes("p.goods_name") %>%
html_text() -> title_data
title_data
install.packages("KoNLP")
# 더이상 R에서 지원x
# 블로그에서 KoNLP zip파일 받아서 R3.6.3의 library에 압축풀기
install.packages("KoNLP")
library(KoNLP)
# 의존모듈 설치: 사전
install.packages("Sejong")
library(KoNLP)
install.packages("hash")
library(KoNLP)
# R이랑 java랑 연동하는 패키지
install.packages("rJava")
library(KoNLP)
install.packages("tau") #테이블셋
install.packages("RSQLite")
library(KoNLP)
install.packages("devtools")
library(KoNLP)
library(stringr)
#### KoNLP의 함수를 테스트 ####
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
#### 분석할 샘플데이터 로딩 ####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments, 10)
head(score, 10)
sampledata <- comments[1:1000]
class(sampledata)
#### KoNLP의 함수를 테스트 ####
# 명사만 추출하는 함수
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data # 2차원 배열과 비슷한 개념
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments, 10)
head(score, 10)
sampledata <- comments[1:1000]
class(sampledata)
#### 형태소 분석을 하기 위해서 명사 분리 ####
# list: 다른 길이의 데이터를 저장할 수 있는 자료구조
# 댓글을 분리하면 분리된 명사의 개수가 다르므로 리스트를 이용
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data # 2차원 배열과 비슷한 개념
}
data_list[[2]]
head(data_list, 20)
readPage %>%
html_nodes("p.goods_name") %>%
html_text() -> title_data
title_data
head(data_list, 20)
# 명사는 한 단어에 하나만 있고, 앞에 위치한다
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,3,3,3,3,3)
), # 반복작업할 데이터
function(x){ #익명 함수
x[1]
} # 반복해서 적용할 함수
)
# 명사는 한 단어에 하나만 있고, 앞에 위치한다
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,4,5,6,7,8)
), # 반복작업할 데이터
function(x){ #익명 함수
x[1]
} # 반복해서 적용할 함수
)
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), # 반복작업할 데이터
#        function(x){ #익명 함수
#          x[1]
#         } # 반복해서 적용할 함수
#        )
# str_split은 기호를 기준으로 분리
wordlist <- sapply(str_split(data_list, "/"), function(x){
x[1]
})
wordlist
#### KoNLP의 함수를 테스트 ####
# 명사만 추출하는 함수
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
#### 분석할 샘플데이터 로딩 ####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments, 10)
head(score, 10)
sampledata <- comments[1:1000]
class(sampledata)
#### 형태소 분석을 하기 위해서 명사 분리 ####
# list: 다른 길이의 데이터를 저장할 수 있는 자료구조
# 댓글을 분리하면 분리된 명사의 개수가 다르므로 리스트를 이용
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data # 2차원 배열과 비슷한 개념
}
data_list[[2]]
head(data_list, 20)
# 명사는 한 단어에 하나만 있고, 앞에 위치한다
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), # 반복작업할 데이터
#        function(x){ #익명 함수
#          x[1]
#         } # 반복해서 적용할 함수
#        )
# str_split은 기호를 기준으로 분리
wordlist <- sapply(str_split(data_list, "/"), function(x){
x[1]
})
head(wordlist,10)
data_list[[2]]
head(data_list, 20)
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), # 반복작업할 데이터
#        function(x){ #익명 함수
#          x[1]
#         } # 반복해서 적용할 함수
#        )
# str_split은 기호를 기준으로 분리
wordlist <- sapply(str_split(data_list, "/"), function(x){
x[1]
})
head(wordlist,10)
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), # 반복작업할 데이터
#        function(x){ #익명 함수
#          x[1]
#         } # 반복해서 적용할 함수
#        )
# str_split은 기호를 기준으로 분리
class(data_list)
class(wordlist)
head(wordlist,10)
#### KoNLP의 함수를 테스트 ####
# 명사만 추출하는 함수
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
#### 분석할 샘플데이터 로딩 ####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments, 10)
head(score, 10)
sampledata <- comments[1:1000]
class(sampledata)
#### 형태소 분석을 하기 위해서 명사 분리 ####
# list: 다른 길이의 데이터를 저장할 수 있는 자료구조
# 댓글을 분리하면 분리된 명사의 개수가 다르므로 리스트를 이용
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data # 2차원 배열과 비슷한 개념
}
data_list[[2]]
head(data_list, 20)
# 명사는 한 단어에 하나만 있고, 앞에 위치한다
# /로 분할 - 리스트의 모든 요소에 저장된 문자열을 /로 분리
#             => N이 있는 문자열의 첫번째 요소 가져오기
# sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), # 반복작업할 데이터
#        function(x){ #익명 함수
#          x[1]
#         } # 반복해서 적용할 함수
#        )
# str_split은 기호를 기준으로 분리
class(data_list) #list
wordlist <- sapply(str_split(data_list, "/"), function(x){
x[1]
})
class(wordlist) #character
head(wordlist,10)
library(KoNLP)
library(stringr)
