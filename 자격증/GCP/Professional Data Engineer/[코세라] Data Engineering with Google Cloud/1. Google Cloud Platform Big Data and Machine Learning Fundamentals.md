# 1. Google Cloud Platform Big Data and Machine Learning Fundamentals

> - Recall the four big data challenges you will learn how to solve
> - Understand the Qwiklabs interactive lab environment that you will be using
> - Identify the critical data roles in an organization
> - Analyze large datasets with BigQuery in your lab

## 1) Introduction to Google Cloud Platform

four big data challenges

1. Migrating existing data workloads (ex: Hadoop, Spark jobs)

   : effectively analyze all of your data.

2. Analyzing large datasets at scale

   : TB ~ PB scale

3. Building streaming data pipelines

   : to make data-driven decisions more quickly.

4. Applying machine learning to your data

   : able to make predictive forward-looking actions.

### Introduction to Google Cloud Platform

원래는 구글 어플리케이션에 적용하던 인프라를 이제 일반에게도 제공한다.

- Compute

  : 미래에 유저와 데이터의 증가를 지원하기 위해 많은 compute power가 필요하다.

  - 특히 머신 러닝에서는 상당한 compute resources가 필요하다.

    - Automatic Video Stabilization for Google Photos
      : 비디오의 이미지 스틸샷(정적인 순간), 폰 자이로스코프, 렌즈 모션 데이터

    -  TPUs (Tensor Processing Units)

      : ML에 특화된 하드웨어. 구글 클라우드를 통해서 다른 비즈니에서도 사용할 수 있다.

    - 구글 데이터센터의 쿨링 비용을 감소시키는데 ML 사용

  **[Demo: Creating a VM on Compute Engine]**

  > USGS의 earthquake 데이터 사용

  1. compute engine instance 작동시키기

     : 메뉴의 [Compute Engine] 항목 -> [VM instances]

     [Create] -> [New VM instance] 

     - 인스턴스 이름: 마음대로
     - 리전: asia-northeast3(서울)
     - 영역: asia-northeast3-a
     - 머신 유형: e2-medium(vCPU 2개, 4GB 메모리)
     - 부팅 디스크: Debian GNU/Linux 10, 표준 영구 디스크 10GB
     - ID 및 API 액세스 > 액세스 범위: 모든 Cloud API에 대한 전체 액세스 허용
       - VM에서 Cloud Storage에 write 할 수 있다.
     - 방화벽: 기본값
       - HTTP/HTTPS 트래픽 허용은 나중에. 지금은 SSH로만 접근할 수 있도록 설정한다.

  2. 그 위에서 소프트웨어 실행하기

     : VM instance의 이름을 클릭하면 ssh로 virtual machine에 접속할 수 있다.

     - git 설치하기

       - `sudo`로 root 권한에 access 할 수 있다.

         ``` bash
         ~$ sudo apt-get install git
         ~$ git clone https://www.github.com/GoogleCloudPlatform/training-data-analyst
         ```

     - bdml_fundamental 폴더로 가서 earthquakevm 폴더로 간다.

       - `ingest.sh` - data를 ingest하는 셸 스크립트 파일. wget 명령어로 URL에서 earthquakes.csv 파일을 다운로드 한다.

         `transform.py` - matplotlib을 이용해 data를 parse해서 Earth map에 뿌려 PNG파일을 생성한다.

         `install_missing.sh` - 필요한 파이썬 패키지를 설치한다.

         ``` bash
         $ ./install_missing.sh
         $ ./ingest.sh # csv파일 다운로드(earthquakes.csv)
         $ ./transform.py # PNG파일 생성(earthquakes.png)
         ```

  3. compute engine instance에서 cloud storage로 파일 복사하기

     : **Compute와 Storage는 분리되어 있으므로 작업이 끝난 후, 완료된 작업물을 Cloud Storage에 올리고 VM은 삭제하는 것을 목표로 한다.**

     메뉴의 [Storage] 항목 -> [Browser]

     [Create a bucket] 

     - bucket name은 globally unique 해야한다.
       - project name도 globally unique하기 때문에 프로젝트 네임을 그대로 사용해도 된다.
     - Multi-Regional - 세계 어디에서든 접속할 수 있다.
     - Set object-level and bucket-level permissions 체크

     `gsutil` 명령어를 통해 조작할 수 있다.

     ``` bash
     $ gsutil ls gs://<버킷이름>
     $ gsutil cp earthquakes.* gs://<버킷이름>
     ```

  4. 그 파일들을 public에 publish하기

     : [REFRESH BUCKET]을 눌러 3개의 파일이 저장된 것을 확인하고, VM을 Stop하거나 Delete을 한다.

     VM 위에서 다른 소프트웨어를 돌릴 예정이지만, 작업이 종료된 후이므로 payment를 지불하기 싫다면 Stop을 하면 된다.

     파일들은 **Not public**으로 설정되어 있다. (파일 소유자만 접근 가능) 

     - 모두가 접근할 수 있도록 public으로 설정하기
       - 원하는 파일을 체크한 뒤 [Permissions] 탭으로 간다.
       - Add members 클릭
         - New members - allUsers 선택
         - Role - Storage Object Viewer
       - 링크를 통해 모두가 볼 수 있다.

     > **결론**: VM이 없어도 파일을 서비스할 수 있다.

- Storage

- Networking

- Security

